{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(num_output_channels=3), transforms.ToTensor()]\n",
    ")\n",
    "train_ds = datasets.MNIST(root=\".\", train=True, transform=transform, download=True)\n",
    "train_dl = DataLoader(train_ds, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=32, padding=1, kernel_size=3, stride=1\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, padding=1, kernel_size=3, stride=2\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=64, padding=1, kernel_size=3, stride=2\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 64, 7, 7)\n",
    "print(x.shape)\n",
    "x = x.permute(0, 2, 3, 1)\n",
    "print(x.shape)\n",
    "x = x.flatten(0, 2)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_embeddings: int = 128, embedding_dim: int = 64, beta: float = 0.25\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.n_embeddings = num_embeddings\n",
    "        self.table = nn.Embedding(\n",
    "            num_embeddings=num_embeddings,\n",
    "            embedding_dim=embedding_dim,\n",
    "        )\n",
    "        # nn.init.normal_(self.table.weight, 0, 0.1)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        batch_size = x.shape[0]\n",
    "        dimension = x.shape[1]\n",
    "        height = x.shape[2]\n",
    "        width = x.shape[3]\n",
    "\n",
    "        x = x.permute(0, 2, 3, 1)  # BxDxHxW -> BxHxWxD\n",
    "        x_flat = x.flatten(0, 2)  # BxHxWxD-> (B*H*W)xD = NxD\n",
    "\n",
    "        distances = (  # (N, K)\n",
    "            x_flat.pow(2).sum(dim=1, keepdim=True)  # (N, D) -> (N, 1)\n",
    "            + self.table.weight.pow(2).sum(dim=1)  # (K, D) -> (K,)\n",
    "            - 2 * x_flat @ self.table.weight.t()  # (N, D) @ (D, K) -> (N, K)\n",
    "        )\n",
    "\n",
    "        indices_flat = distances.argmin(dim=1)  # (N,)\n",
    "        embeddings_flat = self.table(indices_flat)  # (N, D)\n",
    "\n",
    "        z_q_flat = x_flat + (embeddings_flat - x_flat).detach()\n",
    "\n",
    "        codebook_loss = (x_flat.detach() - embeddings_flat).pow(2).mean()\n",
    "        commitment_loss = self.beta * (x_flat - embeddings_flat.detach()).pow(2).mean()\n",
    "        total_loss = codebook_loss + commitment_loss\n",
    "\n",
    "        z_q = z_q_flat.view(batch_size, height, width, dimension)\n",
    "        z_q = z_q.permute(0, 3, 1, 2)\n",
    "\n",
    "        indices = indices_flat.view(batch_size, height, width)  # (N,) -> (B, H, W)\n",
    "\n",
    "        return z_q, indices, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vq = VectorQuantizer()\n",
    "x = torch.randn(32, 64, 7, 7)\n",
    "z_q, indices, loss = vq.forward(x)\n",
    "z_q.shape, indices.shape, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tconv1 = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.tconv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.tconv3 = nn.ConvTranspose2d(32, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.tconv1(x))\n",
    "        x = F.relu(self.tconv2(x))\n",
    "        x = self.tconv3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "x = torch.randn(4, 64, 7, 7)\n",
    "model = Decoder()\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.vq = VectorQuantizer(num_embeddings=128, embedding_dim=64, beta=0.25)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        z_e = self.encoder(x)\n",
    "        z_q, indices, vq_loss = self.vq(z_e)\n",
    "        logits = self.decoder(z_q)\n",
    "        return logits, vq_loss, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAE()\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for x, _ in train_dl:\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, vq_loss, indices = model(x)\n",
    "        # recon_loss = F.binary_cross_entropy_with_logits(logits, x)\n",
    "        recon_loss = criterion(logits, x)\n",
    "        loss = recon_loss + vq_loss\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch}  Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a decoded image and original image side by side\n",
    "index = 78\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(x[index].cpu().numpy().transpose(1, 2, 0))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(logits[index].sigmoid().detach().cpu().numpy().transpose(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
